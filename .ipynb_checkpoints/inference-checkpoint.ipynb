{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    " \n",
    "import scipy.io as sio\n",
    " \n",
    "from torchvision import datasets, transforms \n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models \n",
    "import torchvision.utils as utils  \n",
    "\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import pyrealsense2 as rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device= torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50,ResNet50_Weights\n",
    "net=resnet50(weights=ResNet50_Weights.IMAGENET1K_V2).to(device)\n",
    "\n",
    "### strip the last layer\n",
    "feature_extractor = torch.nn.Sequential(*list(net.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BC(nn.Module):\n",
    "    def __init__(self, input_size=2048, output_size=6):\n",
    "        super(BC, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x=self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  BC().to(device)\n",
    "model.load_state_dict(torch.load('resnet50sawyer_pick_bc_xyz.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRealSense:\n",
    "    def __init__(self):\n",
    "        self.pipe = rs.pipeline()\n",
    "        self.profile = self.pipe.start()\n",
    "\n",
    "    def get_current_frame(self, scale=0.5):\n",
    "        frames = self.pipe.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame() \n",
    "\n",
    "        image=np.asanyarray(color_frame.get_data())\n",
    "        image= cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        image = cv2.resize(image, (int(image.shape[1]*scale), int(image.shape[0]*scale) ) , interpolation = cv2.INTER_AREA)\n",
    "        return image \n",
    "\n",
    "    def close(self):\n",
    "        self.pipe.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam=MyRealSense()\n",
    "\n",
    "while True:\n",
    "    image = cam.get_current_frame()\n",
    "    x=torch.tensor(image.transpose(2,0,1)).float().to(device)[None]\n",
    "    fs=feature_extractor(x).squeeze()\n",
    "    paction=model(fs).detach()\n",
    "\n",
    "    cv2.imshow(\"press q to close\", image)\n",
    "    key = cv2.waitKey(1)\n",
    "    # Press esc or 'q' to close the image window\n",
    "    if key & 0xFF == ord('q') or key == 27:\n",
    "        cv2.destroyAllWindows()\n",
    "        cam.close()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 640, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cv2.destroyAllWindows()\n",
    "# cam.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.45156348,  0.22939338,  0.2790616 , -1.3480388 , -0.00167866,\n",
       "        0.73156327], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paction.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ 0.45156348  0.22939338  0.2790616  -1.3480388  -0.00167866  0.73156327]'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa=paction.cpu().numpy()\n",
    "str(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.45156348,  0.22939338,  0.2790616 , -1.3480388 , -0.00167866,\n",
       "        0.73156327], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "paction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45156348, 0.22939338, 0.2790616, -1.3480388, -0.0016786605, 0.73156327]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ns/miniconda3/lib/python3.10/site-packages/torch/jit/_trace.py:154: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /opt/conda/conda-bld/pytorch_1678411187366/work/build/aten/src/ATen/core/TensorBody.h:486.)\n",
      "  if a.grad is not None:\n"
     ]
    }
   ],
   "source": [
    "traced_model = torch.jit.trace(model.forward,fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model.save(\"traced_bc.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "fn=f\"sawyer_pick_{i}_imgs_xyz.mat\"\n",
    "data=sio.loadmat(fn)\n",
    "actions=data['xyz']\n",
    "imgs=data['imgs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((102, 540, 960, 3), (102, 6))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape, actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(imgs)):\n",
    "    img=imgs[i]\n",
    "    action=actions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((540, 960, 3), (6,))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
